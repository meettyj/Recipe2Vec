{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 21 16:00:36 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    34W / 250W |   1071MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:03:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    31W / 250W |  10905MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0   103W / 250W |   9754MiB / 16280MiB |     95%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  Off  | 00000000:83:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    37W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     13739      C   ...niconda3/4.9.2/bin/python     1069MiB |\r\n",
      "|    1   N/A  N/A     27279      C   ...niconda3/4.9.2/bin/python    10903MiB |\r\n",
      "|    2   N/A  N/A     22342      C   ...niconda3/4.9.2/bin/python     4875MiB |\r\n",
      "|    2   N/A  N/A     22494      C   ...niconda3/4.9.2/bin/python     4877MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4cef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "import heapq\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import lmdb\n",
    "import gensim\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchfile\n",
    "import dgl.function as fn\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "# from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.sampling import RandomWalkNeighborSampler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9ded63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "dataset_folder = '../data/'\n",
    "ckpt_folder = '../data/'\n",
    "tuned_emb_file = 'pretrained_image_emb.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f757c1",
   "metadata": {},
   "source": [
    "## build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5376f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating graph ...\n",
      "graph:  Graph(num_nodes={'ingredient': 22186, 'recipe': 472515, 'user': 38624},\n",
      "      num_edges={('ingredient', 'i-i', 'ingredient'): 170642, ('ingredient', 'i-r', 'recipe'): 4440820, ('recipe', 'r-i', 'ingredient'): 4440820, ('recipe', 'r-r', 'recipe'): 644256, ('recipe', 'r-u', 'user'): 1193179, ('user', 'u-r', 'recipe'): 1193179},\n",
      "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')])\n"
     ]
    }
   ],
   "source": [
    "def get_graph():\n",
    "    print('generating graph ...')\n",
    "    all_r2i_src_dst, train_r2i_src_dst, val_r2i_src_dst, test_r2i_src_dst = torch.load(dataset_folder+'r2i_all_train_val_test_src_dst.pt')\n",
    "    r2i_edge_src, r2i_edge_dst = all_r2i_src_dst\n",
    "    r2r_edge_src, r2r_edge_dst, r2r_edge_weight = torch.load(dataset_folder+'r2r_edge_src_dst_weight.pt')\n",
    "    i2i_edge_src, i2i_edge_dst, i2i_edge_weight = torch.load(dataset_folder+'/i2i_edge_src_dst_weight.pt')\n",
    "    all_u2r_src_dst_weight, train_u2r_src_dst_weight, val_u2r_src_dst_weight, test_u2r_src_dst_weight = torch.load(dataset_folder+'u2r_all_train_val_test_src_dst_weight.pt')\n",
    "    u2r_edge_src, u2r_edge_dst, u2r_edge_weight = all_u2r_src_dst_weight\n",
    "\n",
    "    # nodes and edges\n",
    "    graph = dgl.heterograph({\n",
    "        ('recipe', 'r-i', 'ingredient'): (r2i_edge_src, r2i_edge_dst),\n",
    "        ('ingredient', 'i-r', 'recipe'): (r2i_edge_dst, r2i_edge_src),\n",
    "        ('recipe', 'r-r', 'recipe'): (r2r_edge_src, r2r_edge_dst),\n",
    "        ('ingredient', 'i-i', 'ingredient'): (i2i_edge_src, i2i_edge_dst),\n",
    "        ('user', 'u-r', 'recipe'): (u2r_edge_src, u2r_edge_dst),\n",
    "        ('recipe', 'r-u', 'user'): (u2r_edge_dst, u2r_edge_src)\n",
    "    })\n",
    "\n",
    "    # edge weight\n",
    "    graph.edges['r-r'].data['weight'] = torch.FloatTensor(r2r_edge_weight)\n",
    "    graph.edges['i-i'].data['weight'] = torch.FloatTensor(i2i_edge_weight)\n",
    "    graph.edges['u-r'].data['weight'] = torch.FloatTensor(u2r_edge_weight)\n",
    "    graph.edges['r-u'].data['weight'] = torch.FloatTensor(u2r_edge_weight)\n",
    "    graph.edges['r-i'].data['weight'] = torch.ones(4440820)\n",
    "    graph.edges['i-r'].data['weight'] = torch.ones(4440820)\n",
    "    \n",
    "    # node features\n",
    "    recipe_nodes_avg_instruction_features = torch.load(dataset_folder+'/recipe_nodes_instruction_features.pt')\n",
    "    ingredient_nodes_nutrient_features = torch.load(dataset_folder+'/ingredient_nodes_nutrient_features.pt')\n",
    "    recipe_nodes_pretraind_image_features = torch.load(ckpt_folder + tuned_emb_file)\n",
    "    graph.nodes['recipe'].data['avg_instr_feature'] = recipe_nodes_avg_instruction_features\n",
    "    graph.nodes['recipe'].data['random_instr'] = torch.nn.init.xavier_normal_(torch.ones(472515, 512))\n",
    "    graph.nodes['ingredient'].data['nutrient_feature'] = ingredient_nodes_nutrient_features\n",
    "    graph.nodes['recipe'].data['resnet_image'] = recipe_nodes_pretraind_image_features\n",
    "    graph.nodes['user'].data['random_feature'] = torch.nn.init.xavier_normal_(torch.ones(38624, 300))\n",
    "    graph.nodes['recipe'].data['nodeID'] = graph.nodes('recipe')\n",
    "    \n",
    "    # labels and masks\n",
    "    train_mask = torch.load(dataset_folder+'/train_cuisine_mask.pt')\n",
    "    val_mask = torch.load(dataset_folder+'/val_cuisine_mask.pt')\n",
    "    test_mask = torch.load(dataset_folder+'/test_cuisine_mask.pt')\n",
    "    recipe_nodes_labels = torch.load(dataset_folder+'/recipe_nodes_cuisine_labels.pt')\n",
    "    graph.nodes['recipe'].data['train_mask'] = train_mask\n",
    "    graph.nodes['recipe'].data['val_mask'] = val_mask\n",
    "    graph.nodes['recipe'].data['test_mask'] = test_mask\n",
    "    graph.nodes['recipe'].data['label'] = recipe_nodes_labels.long()\n",
    "\n",
    "    return graph\n",
    "\n",
    "graph = get_graph()\n",
    "print('graph: ', graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47d7620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_idx:  330911\n",
      "length of val_idx:  70448\n",
      "length of test_idx:  71156\n"
     ]
    }
   ],
   "source": [
    "def get_train_val_test_idx():\n",
    "    train_mask = graph.nodes['recipe'].data['train_mask'].to(device)\n",
    "    val_mask = graph.nodes['recipe'].data['val_mask'].to(device)\n",
    "    test_mask = graph.nodes['recipe'].data['test_mask'].to(device)\n",
    "    labels = graph.nodes['recipe'].data['label'].to(device)\n",
    "\n",
    "    train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "    val_idx = torch.nonzero(val_mask, as_tuple=False).squeeze()\n",
    "    test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_idx, val_idx, test_idx = get_train_val_test_idx()\n",
    "\n",
    "print('length of train_idx: ', len(train_idx))\n",
    "print('length of val_idx: ', len(val_idx))\n",
    "print('length of test_idx: ', len(test_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f55d1",
   "metadata": {},
   "source": [
    "## Adversarial Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c457419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PGD_inputs(model, blocks, inputs, labels, seeds, eps=0.02, alpha=0.005, iters=5): # 8. / 255.\n",
    "    # init\n",
    "    user, instr, ingredient, image = inputs\n",
    "    \n",
    "    delta_user = torch.rand([len(user), 128]) * eps * 2 - eps\n",
    "    delta_instr = torch.rand([len(instr), 128]) * eps * 2 - eps\n",
    "    delta_ingredient = torch.rand([len(ingredient), 128]) * eps * 2 - eps\n",
    "    delta_image = torch.rand([len(image), 128]) * eps * 2 - eps\n",
    "    \n",
    "    delta_user = delta_user.to(device)\n",
    "    delta_instr = delta_instr.to(device)\n",
    "    delta_ingredient = delta_ingredient.to(device)\n",
    "    delta_image = delta_image.to(device)\n",
    "    \n",
    "    delta_user = torch.nn.Parameter(delta_user)\n",
    "    delta_instr = torch.nn.Parameter(delta_instr)\n",
    "    delta_ingredient = torch.nn.Parameter(delta_ingredient)\n",
    "    delta_image = torch.nn.Parameter(delta_image)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        h1_instr = model.encoder_instr(global_h_list_instr[blocks[0].srcdata['_ID']]) # hs[0]\n",
    "        h1_instr = norm(h1_instr)\n",
    "        h1_instr = model.mp_gin(blocks[0], h1_instr)\n",
    "        h1_instr = h1_instr.unsqueeze(1)\n",
    "        \n",
    "        h1_image = model.encoder_image(global_h_list_image[blocks[0].srcdata['_ID']]) # hs[0]\n",
    "        h1_image = norm(h1_image)\n",
    "        h1_image = model.mp_gin(blocks[0], h1_image)\n",
    "        h1_image = h1_image.unsqueeze(1)\n",
    "    \n",
    "        p_user = model.user_embedding(user)\n",
    "        p_mid_instr = model.encoder_instr(instr)\n",
    "        p_ingredient = model.ingredient_embedding(ingredient)\n",
    "        p_mid_image = model.encoder_image(image)\n",
    "        \n",
    "        p_user = p_user + delta_user\n",
    "        p_mid_instr = p_mid_instr + delta_instr\n",
    "        p_ingredient = p_ingredient + delta_ingredient\n",
    "        p_mid_image = p_mid_image + delta_image\n",
    "        \n",
    "        p_user = norm(p_user)\n",
    "        p_mid_instr = norm(p_mid_instr)\n",
    "        p_ingredient = norm(p_ingredient)\n",
    "        p_mid_image = norm(p_mid_image)\n",
    "        \n",
    "        x1 = model.rgcn(blocks[-1:], {'user': p_user, 'recipe': p_mid_instr, 'ingredient': p_ingredient})\n",
    "        x2 = model.rgcn(blocks[-1:], {'user': p_user, 'recipe': p_mid_image, 'ingredient': p_ingredient})\n",
    "        \n",
    "        x1 = torch.cat([x1, h1_instr], dim=1)\n",
    "        x2 = torch.cat([x2, h1_image], dim=1)\n",
    "        x = model.cross_view_out(torch.cat([x1, x2], dim=2))\n",
    "        x = model.relation_attention(x)\n",
    "        logits = model.out(x)\n",
    "\n",
    "        # update\n",
    "        model.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # --- delta update ---\n",
    "        # user\n",
    "        delta_user.data = delta_user.data + alpha * delta_user.grad.sign()\n",
    "        delta_user.grad = None\n",
    "        delta_user.data = torch.clamp(delta_user.data, min=-eps, max=eps)\n",
    "        # instr\n",
    "        delta_instr.data = delta_instr.data + alpha * delta_instr.grad.sign()\n",
    "        delta_instr.grad = None\n",
    "        delta_instr.data = torch.clamp(delta_instr.data, min=-eps, max=eps)\n",
    "        # ingredient\n",
    "        delta_ingredient.data = delta_ingredient.data + alpha * delta_ingredient.grad.sign()\n",
    "        delta_ingredient.grad = None\n",
    "        delta_ingredient.data = torch.clamp(delta_ingredient.data, min=-eps, max=eps)\n",
    "        # image\n",
    "        delta_image.data = delta_image.data + alpha * delta_image.grad.sign()\n",
    "        delta_image.grad = None\n",
    "        delta_image.data = torch.clamp(delta_image.data, min=-eps, max=eps)\n",
    "\n",
    "    output = [(delta_user).detach(), (delta_instr).detach(), (delta_ingredient).detach(), (delta_image).detach()]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5abd29",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d65d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_GINConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 apply_func,\n",
    "                 aggregator_type,\n",
    "                 init_eps=0,\n",
    "                 learn_eps=False):\n",
    "        super(custom_GINConv, self).__init__()\n",
    "        self.apply_func = apply_func\n",
    "        self._aggregator_type = aggregator_type\n",
    "        if aggregator_type == 'sum':\n",
    "            self._reducer = fn.sum\n",
    "        elif aggregator_type == 'max':\n",
    "            self._reducer = fn.max\n",
    "        elif aggregator_type == 'mean':\n",
    "            self._reducer = fn.mean\n",
    "        else:\n",
    "            raise KeyError('Aggregator type {} not recognized.'.format(aggregator_type))\n",
    "        # to specify whether eps is trainable or not.\n",
    "        if learn_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.FloatTensor([init_eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.FloatTensor([init_eps]))\n",
    "        \n",
    "        self.fc_src = nn.Linear(128, 128, bias=False)\n",
    "        self.fc_dst = nn.Linear(128, 128, bias=False)\n",
    "        self.attn_l = nn.Parameter(torch.FloatTensor(size=(1, 128)))\n",
    "        self.attn_r = nn.Parameter(torch.FloatTensor(size=(1, 128)))\n",
    "        self.negative_slope = 0.2\n",
    "        self.leaky_relu = nn.LeakyReLU(self.negative_slope)\n",
    "        self.attn_drop = nn.Dropout(0)\n",
    "        self.fc_src2 = nn.Linear(128, 128, bias=False)\n",
    "        self.fc_dst2 = nn.Linear(128, 128, bias=False)\n",
    "        \n",
    "        # init\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
    "        \n",
    "    def forward(self, graph, feat, edge_weight=None):\n",
    "        with graph.local_scope():\n",
    "            aggregate_fn = fn.copy_src('h', 'm')\n",
    "            if edge_weight is not None:\n",
    "                assert edge_weight.shape[0] == graph.number_of_edges()\n",
    "                graph.edata['_edge_weight'] = edge_weight\n",
    "                aggregate_fn = fn.u_mul_e('h', '_edge_weight', 'm')\n",
    "\n",
    "            feat_src_original, feat_dst_original = expand_as_pair(feat, graph)\n",
    "            \n",
    "            # add W\n",
    "            feat_src1 = self.fc_src(feat_src_original)\n",
    "            feat_dst1 = self.fc_dst(feat_dst_original)\n",
    "            \n",
    "            # add a*W*hi*hj\n",
    "            feat_src2 = self.fc_src2(feat_src_original)\n",
    "            feat_dst2 = self.fc_dst2(feat_dst_original)\n",
    "            el = (feat_src2 * self.attn_l).sum(dim=-1)\n",
    "            er = (feat_dst2 * self.attn_r).sum(dim=-1)\n",
    "            graph.srcdata.update({'el': el, 'feat_src2': feat_src2})\n",
    "            graph.dstdata.update({'er': er, 'feat_dst2': feat_dst2})\n",
    "            graph.apply_edges(fn.u_add_v('el', 'er', 'e')) # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
    "            e = self.leaky_relu(graph.edata.pop('e'))\n",
    "            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
    "            \n",
    "            graph.apply_edges(fn.u_mul_e('feat_src2', 'a', 'a_and_src'))\n",
    "            graph.update_all(fn.v_mul_e('feat_dst2', 'a_and_src', 'm'),\n",
    "                             fn.sum('m', 'add_ft'))\n",
    "            \n",
    "            # out\n",
    "            graph.srcdata['h'] = feat_src1\n",
    "            graph.update_all(aggregate_fn, self._reducer('m', 'neigh'))\n",
    "            rst = (1 + self.eps) * feat_dst1 + graph.dstdata['neigh'] + graph.dstdata['add_ft']\n",
    "            if self.apply_func is not None:\n",
    "                rst = self.apply_func(rst)\n",
    "            return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c993d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerRGCN(nn.Module):\n",
    "    def __init__(self, in_feat, hidden_feat, out_feat, rel_names):\n",
    "        super().__init__()\n",
    "        self.gnn_dict = {\n",
    "                rel : custom_GINConv(torch.nn.Linear(128, 128), 'max')\n",
    "                for rel in rel_names\n",
    "            }\n",
    "        self.conv1 = dglnn.HeteroGraphConv(self.gnn_dict, aggregate='stack')\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "#         edge_weight = blocks[0].edata['weight']\n",
    "        x = self.conv1(blocks[0], x) # edge_weight\n",
    "        return x['recipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32659e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def node_drop(feats, drop_rate, training):\n",
    "#     n = feats.shape[0]\n",
    "#     drop_rates = torch.FloatTensor(np.ones(n) * drop_rate)\n",
    "    \n",
    "#     if training:\n",
    "#         masks = torch.bernoulli(1. - drop_rates).unsqueeze(1)\n",
    "#         feats = masks.to(feats.device) * feats / (1. - drop_rate)\n",
    "#     else:\n",
    "#         feats = feats\n",
    "#     return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32172942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同type neighbor的attention\n",
    "class RelationAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=128):\n",
    "        super(RelationAttention, self).__init__()\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.project(z).mean(0)                    # (M, 1)\n",
    "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
    "        out = (beta * z).sum(1)                        # (N, D * K)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5eb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_h_list_instr = graph.ndata['avg_instr_feature']['recipe'].to(device)\n",
    "global_h_list_image = graph.ndata['resnet_image']['recipe'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e198bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata['x'] = x\n",
    "            edge_subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'), etype='u-r')\n",
    "            return edge_subgraph.edata['score'][('user', 'u-r', 'recipe')].squeeze()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # original\n",
    "        self.user_embedding = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.ingredient_embedding = nn.Sequential(\n",
    "            nn.Linear(46, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mp_gin = custom_GINConv(torch.nn.Linear(128, 128), 'max')\n",
    "        self.rgcn = StochasticTwoLayerRGCN(128, 128, 128, graph.etypes)\n",
    "\n",
    "        self.encoder_instr = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.encoder_image = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.cross_view_out = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 9)\n",
    "        )\n",
    "\n",
    "        self.relation_attention = RelationAttention(128)\n",
    "\n",
    "    def forward(self, blocks, input_features, seeds, adv_deltas):\n",
    "        user, avg_instr, ingredient, image = input_features\n",
    "\n",
    "        if adv_deltas:\n",
    "            # print('adv ...')\n",
    "            delta_user, delta_instr, delta_ingredient, delta_image = adv_deltas\n",
    "            \n",
    "        # metapath\n",
    "        h1_instr = self.encoder_instr(global_h_list_instr[blocks[0].srcdata['_ID']])\n",
    "        h1_image = self.encoder_image(global_h_list_image[blocks[0].srcdata['_ID']])\n",
    "        h1_instr = norm(h1_instr)\n",
    "        h1_image = norm(h1_image)\n",
    "        h1_instr = self.mp_gin(blocks[0], h1_instr)\n",
    "        h1_image = self.mp_gin(blocks[0], h1_image)\n",
    "        h1_instr = h1_instr.unsqueeze(1)\n",
    "        h1_image = h1_image.unsqueeze(1)\n",
    "        \n",
    "        # schema\n",
    "        user = self.user_embedding(user)\n",
    "        ingredient = self.ingredient_embedding(ingredient)\n",
    "        mid_instr = self.encoder_instr(avg_instr)\n",
    "        mid_image = self.encoder_image(image)\n",
    "        \n",
    "        if adv_deltas:\n",
    "            user += delta_user\n",
    "            ingredient += delta_ingredient\n",
    "            mid_instr += delta_instr\n",
    "            mid_image += delta_image\n",
    "        \n",
    "        user = norm(user)\n",
    "        ingredient = norm(ingredient)\n",
    "        mid_instr = norm(mid_instr)\n",
    "        mid_image = norm(mid_image)\n",
    "    \n",
    "        x1 = self.rgcn(blocks[-1:], {'user': user, 'recipe': mid_instr, 'ingredient': ingredient})\n",
    "        x2 = self.rgcn(blocks[-1:], {'user': user, 'recipe': mid_image, 'ingredient': ingredient})\n",
    "\n",
    "        x1 = torch.cat([x1, h1_instr], dim=1)\n",
    "        x2 = torch.cat([x2, h1_image], dim=1)\n",
    "        x = self.cross_view_out(torch.cat([x1, x2], dim=2))\n",
    "        x = self.relation_attention(x)\n",
    "  \n",
    "        return self.out(x), x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083768fa",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "334f6c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Block(num_src_nodes=39876, num_dst_nodes=4096, num_edges=39629), Block(num_src_nodes={'ingredient': 3947, 'recipe': 7207, 'user': 4561},\n",
      "      num_dst_nodes={'ingredient': 0, 'recipe': 4096, 'user': 0},\n",
      "      num_edges={('ingredient', 'i-i', 'ingredient'): 0, ('ingredient', 'i-r', 'recipe'): 38179, ('recipe', 'r-i', 'ingredient'): 0, ('recipe', 'r-r', 'recipe'): 3652, ('recipe', 'r-u', 'user'): 0, ('user', 'u-r', 'recipe'): 10092},\n",
      "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r'), ('recipe', 'user', 'r-u'), ('user', 'recipe', 'u-r')])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metapath_list = [['r-u', 'u-r']]\n",
    "\n",
    "class HANSampler(object):\n",
    "    def __init__(self, g, metapath_list, num_neighbors):\n",
    "        self.sampler_list = []\n",
    "        for metapath in metapath_list:\n",
    "            # note: random walk may get same route(same edge), which will be removed in the sampled graph.\n",
    "            # So the sampled graph's edges may be less than num_random_walks(num_neighbors).\n",
    "            self.sampler_list.append(RandomWalkNeighborSampler(G=g,\n",
    "                                                               num_traversals=2,\n",
    "                                                               termination_prob=0,\n",
    "                                                               num_random_walks=num_neighbors,\n",
    "                                                               num_neighbors=num_neighbors,\n",
    "                                                               metapath=metapath))\n",
    "            \n",
    "            self.schema_sampler = dgl.dataloading.MultiLayerNeighborSampler([20])\n",
    "\n",
    "    def sample_blocks(self, seeds):\n",
    "        block_list = []\n",
    "        seeds = torch.stack(seeds)\n",
    "        \n",
    "        sampled_mp_src_nodes = []\n",
    "        sampled_mp_dst_nodes = []\n",
    "        for sampler in self.sampler_list:\n",
    "            frontier = sampler(seeds)\n",
    "            frontier = dgl.remove_self_loop(frontier)\n",
    "            block = dgl.to_block(frontier, seeds)\n",
    "            block_list.append(block)\n",
    "        \n",
    "        # schema\n",
    "        schema_NodeDataLoader = dgl.dataloading.NodeDataLoader(graph, {'recipe': seeds}, self.schema_sampler,\n",
    "                                        batch_size=4096, shuffle=False, drop_last=False, num_workers=0)\n",
    "        for input_nodes, output_nodes, blocks in schema_NodeDataLoader:\n",
    "            block_list.append(blocks[0])\n",
    "            break\n",
    "\n",
    "        return seeds, block_list\n",
    "    \n",
    "han_sampler = HANSampler(graph, metapath_list, num_neighbors=10)\n",
    "train_dataloader = DataLoader(\n",
    "                    dataset=train_idx.cpu(),\n",
    "                    batch_size=4096,\n",
    "                    collate_fn=han_sampler.sample_blocks,\n",
    "                    shuffle=True,\n",
    "                    drop_last=False,\n",
    "                    num_workers=4)\n",
    "\n",
    "for step, (seeds, blocks) in enumerate(train_dataloader):\n",
    "    print(blocks)\n",
    "    break\n",
    "print()\n",
    "    \n",
    "han_val_sampler = HANSampler(graph, metapath_list, num_neighbors=10)\n",
    "val_dataloader = DataLoader(\n",
    "                dataset=val_idx.cpu(),\n",
    "                batch_size=4096,\n",
    "                collate_fn=han_val_sampler.sample_blocks,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                num_workers=0)\n",
    "    \n",
    "han_test_sampler = HANSampler(graph, metapath_list, num_neighbors=10)\n",
    "test_dataloader = DataLoader(\n",
    "                    dataset=test_idx.cpu(),\n",
    "                    batch_size=4096,\n",
    "                    collate_fn=han_test_sampler.sample_blocks,\n",
    "                    shuffle=False,\n",
    "                    drop_last=False,\n",
    "                    num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d8256",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbc0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(input, p=1, dim=1, eps=1e-12):\n",
    "    return input / input.norm(p, dim, keepdim=True).clamp(min=eps).expand_as(input)\n",
    "\n",
    "\n",
    "def get_score(y_pred, y_true):\n",
    "    total_acc = accuracy_score(y_true, y_pred)\n",
    "    score = {\n",
    "        \"f1\": f1_score(y_true, y_pred, labels=[1, 2, 3, 4, 5, 6, 7, 8], average='micro'),\n",
    "        \"acc\": total_acc\n",
    "    }\n",
    "    \n",
    "    # detailed score\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    detailed_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    detailed_score = {\n",
    "        \"f1\": f1_score(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8], average=None, zero_division=0),\n",
    "        \"acc\": detailed_acc\n",
    "    }\n",
    "    return score, detailed_score\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    # print('evaluating ... ')\n",
    "    evaluate_start = time.time()\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    cosine_total_loss = 0\n",
    "    link_prediction_total_loss = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    detailed_precision = 0\n",
    "    detailed_recall = 0\n",
    "    detailed_f1 = 0\n",
    "    count = 0\n",
    "    \n",
    "    all_y_preds = None\n",
    "    all_labels = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (seeds, blocks) in enumerate(dataloader):\n",
    "            blocks = [blk.to(device) for blk in blocks]\n",
    "        \n",
    "            # input\n",
    "            input_user = blocks[-1].srcdata['random_feature']['user']\n",
    "            input_instr = blocks[-1].srcdata['avg_instr_feature']['recipe'] # avg_instr_feature\n",
    "            input_ingredient = blocks[-1].srcdata['nutrient_feature']['ingredient']\n",
    "            input_image = blocks[-1].srcdata['resnet_image']['recipe']\n",
    "            labels = blocks[-1].dstdata['label']['recipe']\n",
    "\n",
    "            inputs = [input_user, input_instr, input_ingredient, input_image]\n",
    "            logits, _ = model(blocks, inputs, seeds, None)\n",
    "            y_pred = np.argmax(logits.cpu(), axis=1)\n",
    "            \n",
    "            if all_y_preds is None:\n",
    "                all_y_preds = y_pred\n",
    "                all_labels = labels.cpu().numpy()\n",
    "            else:\n",
    "                all_y_preds = np.append(all_y_preds, y_pred, axis=0)\n",
    "                all_labels = np.append(all_labels, labels.cpu().numpy(), axis=0)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            count += len(labels)\n",
    "#             break\n",
    "        \n",
    "        total_score, detailed_score = get_score(all_y_preds, all_labels)\n",
    "        total_f1 = total_score['f1']\n",
    "        total_acc = total_score['acc']\n",
    "        detailed_f1 = detailed_score['f1']\n",
    "        detailed_acc = detailed_score['acc']\n",
    "        \n",
    "        total_loss /= count\n",
    "        link_prediction_total_loss /= count\n",
    "        evalutate_time = time.strftime(\"%M:%S min\", time.gmtime(time.time()-evaluate_start))\n",
    "        \n",
    "    return total_loss, total_f1, total_acc, evalutate_time, detailed_f1, detailed_acc, link_prediction_total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ecb0c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c1a86c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ... \n",
      "Epoch: 0, L: 1.2557, adv_l: 1.2512, T: 01:23 min, LR: 0.005000\n",
      "Testing: \n",
      "Total Loss: 0.0002,  F1: 0.819867, Acc: 0.804542,  Time: 00:06 min, LR: 0.004750\n",
      "detailed_f1:  70.2 & 87.9 & 75.6 & 77.6 & 78.3 & 89.9 & 64.9 & 85.8 & 72.4 & 82.0\n",
      "detailed_acc:  64.2 & 88.8 & 69.4 & 70.6 & 78.4 & 92.3 & 67.7 & 88.8 & 71.1 & 80.5\n",
      "\n",
      "Epoch: 1, L: 0.6031, adv_l: 0.6262, T: 01:22 min, LR: 0.004750\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.861190, Acc: 0.843555,  Time: 00:05 min, LR: 0.004513\n",
      "detailed_f1:  77.4 & 91.7 & 83.9 & 82.3 & 82.7 & 92.3 & 77.7 & 87.1 & 74.2 & 86.1\n",
      "detailed_acc:  73.1 & 94.5 & 79.0 & 82.8 & 86.9 & 95.4 & 74.0 & 91.9 & 67.4 & 84.4\n",
      "\n",
      "Epoch: 2, L: 0.5238, adv_l: 0.5459, T: 01:20 min, LR: 0.004513\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.874189, Acc: 0.856274,  Time: 00:05 min, LR: 0.004287\n",
      "detailed_f1:  78.7 & 93.8 & 85.6 & 85.5 & 85.1 & 93.1 & 79.0 & 88.1 & 76.0 & 87.4\n",
      "detailed_acc:  75.3 & 93.4 & 83.4 & 82.5 & 85.1 & 94.8 & 74.8 & 92.2 & 73.4 & 85.6\n",
      "\n",
      "Epoch: 3, L: 0.4898, adv_l: 0.5078, T: 01:21 min, LR: 0.004287\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.877711, Acc: 0.860180,  Time: 00:05 min, LR: 0.004073\n",
      "detailed_f1:  79.5 & 93.7 & 86.6 & 85.8 & 84.8 & 93.6 & 79.6 & 88.4 & 76.4 & 87.8\n",
      "detailed_acc:  76.1 & 95.6 & 84.1 & 86.4 & 88.5 & 94.8 & 74.7 & 92.3 & 72.2 & 86.0\n",
      "\n",
      "Epoch: 4, L: 0.4719, adv_l: 0.4867, T: 01:20 min, LR: 0.004073\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.881504, Acc: 0.863497,  Time: 00:06 min, LR: 0.003869\n",
      "detailed_f1:  80.0 & 94.1 & 87.3 & 86.6 & 85.8 & 93.7 & 80.0 & 88.6 & 76.9 & 88.2\n",
      "detailed_acc:  77.7 & 95.4 & 86.8 & 84.9 & 86.1 & 94.1 & 74.5 & 91.6 & 75.8 & 86.3\n",
      "\n",
      "Epoch: 5, L: 0.4577, adv_l: 0.4698, T: 01:22 min, LR: 0.003869\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.883660, Acc: 0.866209,  Time: 00:05 min, LR: 0.003675\n",
      "detailed_f1:  80.8 & 94.4 & 87.1 & 86.2 & 85.9 & 93.8 & 80.8 & 88.9 & 77.3 & 88.4\n",
      "detailed_acc:  77.4 & 94.6 & 82.7 & 88.6 & 88.5 & 95.1 & 80.8 & 91.0 & 74.5 & 86.6\n",
      "\n",
      "Epoch: 6, L: 0.4489, adv_l: 0.4581, T: 01:21 min, LR: 0.003675\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.885102, Acc: 0.867292,  Time: 00:05 min, LR: 0.003492\n",
      "detailed_f1:  80.9 & 94.9 & 87.7 & 87.3 & 86.3 & 93.9 & 81.1 & 88.7 & 76.7 & 88.5\n",
      "detailed_acc:  80.6 & 94.4 & 84.9 & 84.9 & 88.3 & 95.2 & 78.1 & 93.1 & 71.1 & 86.7\n",
      "\n",
      "Epoch: 7, L: 0.4402, adv_l: 0.4475, T: 01:21 min, LR: 0.003492\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.886240, Acc: 0.869217,  Time: 00:05 min, LR: 0.003317\n",
      "detailed_f1:  81.3 & 94.7 & 87.1 & 87.3 & 86.3 & 93.7 & 81.2 & 89.2 & 77.8 & 88.6\n",
      "detailed_acc:  77.6 & 95.7 & 82.8 & 86.4 & 88.9 & 96.3 & 82.3 & 90.7 & 74.8 & 86.9\n",
      "\n",
      "Epoch: 8, L: 0.4353, adv_l: 0.4407, T: 01:21 min, LR: 0.003317\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.885956, Acc: 0.868908,  Time: 00:05 min, LR: 0.003151\n",
      "detailed_f1:  81.5 & 95.1 & 87.0 & 87.2 & 86.1 & 93.9 & 80.8 & 88.9 & 77.8 & 88.6\n",
      "detailed_acc:  78.2 & 95.1 & 81.7 & 83.8 & 89.3 & 96.0 & 76.9 & 92.5 & 74.9 & 86.9\n",
      "\n",
      "Epoch: 9, L: 0.4294, adv_l: 0.4335, T: 01:22 min, LR: 0.003151\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.888555, Acc: 0.871437,  Time: 00:05 min, LR: 0.002994\n",
      "detailed_f1:  81.3 & 95.2 & 87.9 & 87.2 & 86.9 & 94.0 & 81.8 & 89.2 & 78.0 & 88.9\n",
      "detailed_acc:  80.2 & 94.9 & 84.8 & 88.8 & 88.4 & 96.2 & 81.6 & 89.9 & 75.6 & 87.1\n",
      "\n",
      "Epoch: 10, L: 0.4226, adv_l: 0.4259, T: 01:23 min, LR: 0.002994\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.889089, Acc: 0.872126,  Time: 00:05 min, LR: 0.002844\n",
      "detailed_f1:  81.3 & 95.5 & 88.3 & 87.4 & 87.0 & 94.1 & 81.8 & 89.2 & 78.0 & 88.9\n",
      "detailed_acc:  83.3 & 94.3 & 88.9 & 84.7 & 87.7 & 94.4 & 79.7 & 91.7 & 74.4 & 87.2\n",
      "\n",
      "Epoch: 11, L: 0.4198, adv_l: 0.4221, T: 01:23 min, LR: 0.002844\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.889692, Acc: 0.872421,  Time: 00:05 min, LR: 0.002702\n",
      "detailed_f1:  81.8 & 95.0 & 88.3 & 87.6 & 86.9 & 94.0 & 81.6 & 89.3 & 77.8 & 89.0\n",
      "detailed_acc:  79.8 & 92.8 & 86.6 & 85.7 & 88.3 & 95.9 & 81.0 & 92.4 & 73.5 & 87.2\n",
      "\n",
      "Epoch: 12, L: 0.4150, adv_l: 0.4166, T: 01:22 min, LR: 0.002702\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.890203, Acc: 0.872815,  Time: 00:05 min, LR: 0.002567\n",
      "detailed_f1:  81.9 & 95.3 & 88.4 & 87.6 & 86.6 & 94.2 & 81.6 & 89.2 & 77.8 & 89.0\n",
      "detailed_acc:  80.3 & 94.4 & 86.5 & 85.1 & 88.8 & 96.2 & 78.7 & 92.3 & 73.9 & 87.3\n",
      "\n",
      "Epoch: 13, L: 0.4106, adv_l: 0.4124, T: 01:23 min, LR: 0.002567\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891441, Acc: 0.874403,  Time: 00:05 min, LR: 0.002438\n",
      "detailed_f1:  81.8 & 95.4 & 88.5 & 88.0 & 86.9 & 94.2 & 82.1 & 89.4 & 78.2 & 89.1\n",
      "detailed_acc:  82.1 & 94.5 & 87.1 & 87.4 & 88.5 & 96.1 & 80.1 & 91.0 & 74.9 & 87.4\n",
      "\n",
      "Epoch: 14, L: 0.4078, adv_l: 0.4093, T: 01:21 min, LR: 0.002438\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891781, Acc: 0.874571,  Time: 00:05 min, LR: 0.002316\n",
      "detailed_f1:  81.8 & 95.2 & 88.4 & 87.9 & 86.9 & 94.2 & 82.1 & 89.5 & 78.1 & 89.2\n",
      "detailed_acc:  81.3 & 96.1 & 86.5 & 88.5 & 87.8 & 96.2 & 79.5 & 91.4 & 74.7 & 87.5\n",
      "\n",
      "Epoch: 15, L: 0.4052, adv_l: 0.4069, T: 01:22 min, LR: 0.002316\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.890547, Acc: 0.873124,  Time: 00:05 min, LR: 0.002201\n",
      "detailed_f1:  82.2 & 95.0 & 88.6 & 87.7 & 86.7 & 94.3 & 81.9 & 89.1 & 77.6 & 89.1\n",
      "detailed_acc:  81.1 & 96.1 & 86.7 & 88.0 & 87.8 & 95.6 & 78.7 & 92.9 & 72.5 & 87.3\n",
      "\n",
      "Epoch: 16, L: 0.4018, adv_l: 0.4028, T: 01:21 min, LR: 0.002201\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891354, Acc: 0.874192,  Time: 00:06 min, LR: 0.002091\n",
      "detailed_f1:  82.1 & 95.4 & 88.4 & 87.8 & 86.8 & 94.2 & 82.1 & 89.3 & 78.1 & 89.1\n",
      "detailed_acc:  77.9 & 95.6 & 86.3 & 87.9 & 87.9 & 96.4 & 80.0 & 92.6 & 74.2 & 87.4\n",
      "\n",
      "Epoch: 17, L: 0.3995, adv_l: 0.4008, T: 01:23 min, LR: 0.002091\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.890457, Acc: 0.872871,  Time: 00:05 min, LR: 0.001986\n",
      "detailed_f1:  82.0 & 95.5 & 88.5 & 87.7 & 86.9 & 94.4 & 81.4 & 89.1 & 77.7 & 89.0\n",
      "detailed_acc:  81.9 & 95.6 & 86.8 & 88.1 & 86.1 & 95.2 & 76.8 & 93.1 & 73.6 & 87.3\n",
      "\n",
      "Epoch: 18, L: 0.3973, adv_l: 0.3983, T: 01:22 min, LR: 0.001986\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891957, Acc: 0.874614,  Time: 00:05 min, LR: 0.001887\n",
      "detailed_f1:  82.0 & 95.5 & 88.7 & 87.8 & 86.7 & 94.2 & 82.1 & 89.4 & 78.2 & 89.2\n",
      "detailed_acc:  79.0 & 95.3 & 86.9 & 87.5 & 88.8 & 95.9 & 79.6 & 91.9 & 75.3 & 87.5\n",
      "\n",
      "Epoch: 19, L: 0.3940, adv_l: 0.3958, T: 01:20 min, LR: 0.001887\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891609, Acc: 0.874136,  Time: 00:05 min, LR: 0.001792\n",
      "detailed_f1:  81.4 & 95.6 & 88.5 & 88.1 & 86.7 & 94.4 & 81.7 & 89.4 & 78.2 & 89.2\n",
      "detailed_acc:  75.3 & 94.9 & 88.0 & 87.5 & 87.9 & 95.7 & 80.4 & 92.1 & 76.7 & 87.4\n",
      "\n",
      "Epoch: 20, L: 0.3914, adv_l: 0.3935, T: 01:20 min, LR: 0.001792\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891762, Acc: 0.874234,  Time: 00:05 min, LR: 0.001703\n",
      "detailed_f1:  82.2 & 95.5 & 88.5 & 87.8 & 87.1 & 94.3 & 81.9 & 89.3 & 77.7 & 89.2\n",
      "detailed_acc:  79.2 & 95.4 & 88.0 & 86.8 & 87.2 & 95.7 & 81.4 & 93.1 & 72.7 & 87.4\n",
      "\n",
      "Epoch: 21, L: 0.3893, adv_l: 0.3915, T: 01:21 min, LR: 0.001703\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892058, Acc: 0.874852,  Time: 00:05 min, LR: 0.001618\n",
      "detailed_f1:  82.2 & 95.6 & 88.7 & 87.9 & 86.7 & 94.5 & 81.9 & 89.3 & 78.1 & 89.2\n",
      "detailed_acc:  79.2 & 95.3 & 88.4 & 86.2 & 89.8 & 94.9 & 79.7 & 92.9 & 74.4 & 87.5\n",
      "\n",
      "Epoch: 22, L: 0.3880, adv_l: 0.3903, T: 01:20 min, LR: 0.001618\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892551, Acc: 0.875288,  Time: 00:05 min, LR: 0.001537\n",
      "detailed_f1:  82.2 & 95.7 & 88.9 & 87.7 & 86.9 & 94.4 & 82.0 & 89.4 & 78.1 & 89.3\n",
      "detailed_acc:  80.9 & 95.5 & 87.9 & 85.2 & 89.2 & 95.9 & 80.3 & 92.0 & 74.4 & 87.5\n",
      "\n",
      "Epoch: 23, L: 0.3856, adv_l: 0.3883, T: 01:20 min, LR: 0.001537\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892808, Acc: 0.875555,  Time: 00:05 min, LR: 0.001460\n",
      "detailed_f1:  82.3 & 95.5 & 88.6 & 87.7 & 87.2 & 94.4 & 82.1 & 89.4 & 78.3 & 89.3\n",
      "detailed_acc:  79.3 & 95.7 & 86.8 & 85.6 & 87.1 & 96.2 & 79.6 & 92.7 & 75.0 & 87.6\n",
      "\n",
      "Epoch: 24, L: 0.3843, adv_l: 0.3875, T: 01:20 min, LR: 0.001460\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892973, Acc: 0.875752,  Time: 00:05 min, LR: 0.001387\n",
      "detailed_f1:  82.2 & 95.8 & 88.8 & 87.8 & 86.7 & 94.5 & 82.2 & 89.5 & 78.4 & 89.3\n",
      "detailed_acc:  80.0 & 95.4 & 87.0 & 88.3 & 89.0 & 95.6 & 81.2 & 91.4 & 75.7 & 87.6\n",
      "\n",
      "Epoch: 25, L: 0.3826, adv_l: 0.3861, T: 01:20 min, LR: 0.001387\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.891149, Acc: 0.873911,  Time: 00:05 min, LR: 0.001318\n",
      "detailed_f1:  81.9 & 95.7 & 88.4 & 87.8 & 87.4 & 94.3 & 81.9 & 89.3 & 77.6 & 89.1\n",
      "detailed_acc:  83.3 & 95.7 & 90.1 & 86.7 & 87.0 & 95.2 & 80.6 & 92.5 & 71.5 & 87.4\n",
      "\n",
      "Epoch: 26, L: 0.3798, adv_l: 0.3837, T: 01:20 min, LR: 0.001318\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892948, Acc: 0.875963,  Time: 00:06 min, LR: 0.001252\n",
      "detailed_f1:  82.2 & 95.5 & 89.0 & 87.7 & 86.9 & 94.5 & 82.1 & 89.4 & 78.5 & 89.3\n",
      "detailed_acc:  78.9 & 95.9 & 87.3 & 86.8 & 88.8 & 95.5 & 80.2 & 92.5 & 75.4 & 87.6\n",
      "\n",
      "Epoch: 27, L: 0.3779, adv_l: 0.3824, T: 01:21 min, LR: 0.001252\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893119, Acc: 0.876145,  Time: 00:05 min, LR: 0.001189\n",
      "detailed_f1:  82.3 & 95.4 & 88.6 & 87.3 & 87.4 & 94.4 & 82.4 & 89.5 & 78.3 & 89.3\n",
      "detailed_acc:  81.0 & 94.6 & 86.0 & 88.3 & 88.1 & 96.3 & 79.9 & 92.6 & 74.0 & 87.6\n",
      "\n",
      "Epoch: 28, L: 0.3765, adv_l: 0.3813, T: 01:21 min, LR: 0.001189\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892403, Acc: 0.874122,  Time: 00:06 min, LR: 0.001130\n",
      "detailed_f1:  82.1 & 95.3 & 88.6 & 87.8 & 87.1 & 94.4 & 82.1 & 89.4 & 78.0 & 89.2\n",
      "detailed_acc:  79.0 & 93.8 & 87.0 & 87.3 & 86.4 & 95.0 & 81.3 & 91.3 & 77.8 & 87.4\n",
      "\n",
      "Epoch: 29, L: 0.3754, adv_l: 0.3805, T: 01:21 min, LR: 0.001130\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893424, Acc: 0.876061,  Time: 00:06 min, LR: 0.001073\n",
      "detailed_f1:  82.3 & 95.6 & 88.9 & 87.5 & 87.2 & 94.5 & 82.4 & 89.4 & 78.3 & 89.3\n",
      "detailed_acc:  79.8 & 95.3 & 87.7 & 88.3 & 87.8 & 95.7 & 81.2 & 91.9 & 75.3 & 87.6\n",
      "\n",
      "Epoch: 30, L: 0.3736, adv_l: 0.3793, T: 01:21 min, LR: 0.001073\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893311, Acc: 0.876103,  Time: 00:05 min, LR: 0.001020\n",
      "detailed_f1:  82.4 & 95.0 & 88.9 & 87.8 & 87.3 & 94.5 & 82.2 & 89.5 & 78.2 & 89.3\n",
      "detailed_acc:  79.8 & 96.8 & 87.0 & 86.4 & 88.3 & 95.9 & 81.4 & 92.5 & 74.1 & 87.6\n",
      "\n",
      "Epoch: 31, L: 0.3731, adv_l: 0.3790, T: 01:21 min, LR: 0.001020\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.890908, Acc: 0.873447,  Time: 00:06 min, LR: 0.000969\n",
      "detailed_f1:  81.8 & 94.9 & 88.4 & 87.6 & 86.6 & 94.4 & 81.7 & 89.4 & 78.0 & 89.1\n",
      "detailed_acc:  77.4 & 96.6 & 85.0 & 88.0 & 88.4 & 95.5 & 82.1 & 92.0 & 74.9 & 87.3\n",
      "\n",
      "Epoch: 32, L: 0.3710, adv_l: 0.3773, T: 01:22 min, LR: 0.000969\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893649, Acc: 0.875906,  Time: 00:06 min, LR: 0.000920\n",
      "detailed_f1:  82.5 & 95.6 & 88.7 & 88.0 & 87.3 & 94.4 & 82.2 & 89.6 & 78.3 & 89.4\n",
      "detailed_acc:  80.3 & 95.9 & 85.8 & 86.9 & 87.5 & 95.3 & 82.1 & 91.2 & 77.0 & 87.6\n",
      "\n",
      "Epoch: 33, L: 0.3690, adv_l: 0.3756, T: 01:21 min, LR: 0.000920\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892705, Acc: 0.875049,  Time: 00:05 min, LR: 0.000874\n",
      "detailed_f1:  82.2 & 95.4 & 88.9 & 87.8 & 86.7 & 94.4 & 82.2 & 89.5 & 78.2 & 89.3\n",
      "detailed_acc:  79.1 & 95.7 & 88.5 & 87.4 & 89.4 & 94.4 & 80.9 & 91.8 & 76.2 & 87.5\n",
      "\n",
      "Epoch: 34, L: 0.3686, adv_l: 0.3758, T: 01:21 min, LR: 0.000874\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893224, Acc: 0.876033,  Time: 00:05 min, LR: 0.000830\n",
      "detailed_f1:  82.0 & 95.5 & 89.0 & 88.0 & 86.8 & 94.5 & 82.4 & 89.5 & 78.4 & 89.3\n",
      "detailed_acc:  82.3 & 96.0 & 88.2 & 87.1 & 88.3 & 95.8 & 79.9 & 91.2 & 75.3 & 87.6\n",
      "\n",
      "Epoch: 35, L: 0.3669, adv_l: 0.3743, T: 01:21 min, LR: 0.000830\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893245, Acc: 0.875710,  Time: 00:05 min, LR: 0.000789\n",
      "detailed_f1:  82.2 & 95.6 & 88.8 & 87.8 & 87.2 & 94.5 & 82.2 & 89.4 & 78.4 & 89.3\n",
      "detailed_acc:  79.0 & 95.7 & 87.9 & 87.7 & 87.4 & 94.9 & 79.3 & 92.1 & 77.0 & 87.6\n",
      "\n",
      "Epoch: 36, L: 0.3654, adv_l: 0.3733, T: 01:19 min, LR: 0.000789\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892344, Acc: 0.874796,  Time: 00:05 min, LR: 0.000749\n",
      "detailed_f1:  82.1 & 95.5 & 88.6 & 88.0 & 86.9 & 94.4 & 82.1 & 89.4 & 78.3 & 89.2\n",
      "detailed_acc:  79.1 & 95.7 & 88.5 & 87.7 & 88.9 & 94.2 & 82.0 & 91.2 & 76.9 & 87.5\n",
      "\n",
      "Epoch: 37, L: 0.3651, adv_l: 0.3732, T: 01:20 min, LR: 0.000749\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.892636, Acc: 0.874712,  Time: 00:06 min, LR: 0.000712\n",
      "detailed_f1:  82.1 & 95.5 & 88.9 & 87.8 & 87.2 & 94.4 & 82.2 & 89.4 & 78.1 & 89.3\n",
      "detailed_acc:  80.3 & 94.9 & 87.8 & 87.5 & 88.1 & 94.8 & 81.5 & 91.0 & 76.9 & 87.5\n",
      "\n",
      "Epoch: 38, L: 0.3635, adv_l: 0.3721, T: 01:21 min, LR: 0.000712\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893046, Acc: 0.876019,  Time: 00:05 min, LR: 0.000676\n",
      "detailed_f1:  82.3 & 95.6 & 88.6 & 87.8 & 87.2 & 94.4 & 82.2 & 89.5 & 78.4 & 89.3\n",
      "detailed_acc:  80.2 & 95.1 & 89.4 & 86.0 & 88.1 & 95.4 & 81.4 & 92.0 & 75.1 & 87.6\n",
      "\n",
      "Epoch: 39, L: 0.3625, adv_l: 0.3715, T: 01:20 min, LR: 0.000676\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893365, Acc: 0.875485,  Time: 00:06 min, LR: 0.000643\n",
      "detailed_f1:  82.1 & 95.5 & 88.6 & 87.8 & 86.9 & 94.4 & 82.2 & 89.6 & 78.3 & 89.3\n",
      "detailed_acc:  77.5 & 95.3 & 87.0 & 87.2 & 88.2 & 95.5 & 81.1 & 91.5 & 77.6 & 87.5\n",
      "\n",
      "Epoch: 40, L: 0.3614, adv_l: 0.3707, T: 01:21 min, LR: 0.000643\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893230, Acc: 0.875780,  Time: 00:05 min, LR: 0.000610\n",
      "detailed_f1:  82.3 & 95.5 & 88.9 & 87.9 & 87.2 & 94.5 & 82.1 & 89.5 & 78.3 & 89.3\n",
      "detailed_acc:  80.3 & 95.2 & 88.4 & 86.6 & 88.2 & 94.9 & 80.2 & 92.0 & 76.1 & 87.6\n",
      "\n",
      "Epoch: 41, L: 0.3608, adv_l: 0.3704, T: 01:20 min, LR: 0.000610\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893762, Acc: 0.876553,  Time: 00:05 min, LR: 0.000580\n",
      "detailed_f1:  82.5 & 95.5 & 89.0 & 87.8 & 87.1 & 94.5 & 82.2 & 89.6 & 78.5 & 89.4\n",
      "detailed_acc:  80.1 & 94.5 & 87.5 & 88.4 & 89.1 & 95.9 & 81.8 & 90.8 & 76.5 & 87.7\n",
      "\n",
      "Epoch: 42, L: 0.3595, adv_l: 0.3695, T: 01:20 min, LR: 0.000580\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893854, Acc: 0.876665,  Time: 00:06 min, LR: 0.000551\n",
      "detailed_f1:  82.4 & 95.5 & 89.0 & 88.0 & 87.3 & 94.5 & 82.2 & 89.5 & 78.5 & 89.4\n",
      "detailed_acc:  80.3 & 95.1 & 88.1 & 87.0 & 87.8 & 95.4 & 81.3 & 92.0 & 75.7 & 87.7\n",
      "\n",
      "Epoch: 43, L: 0.3586, adv_l: 0.3690, T: 01:20 min, LR: 0.000551\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893389, Acc: 0.876061,  Time: 00:06 min, LR: 0.000523\n",
      "detailed_f1:  82.4 & 95.5 & 88.7 & 87.8 & 87.1 & 94.5 & 82.1 & 89.5 & 78.4 & 89.3\n",
      "detailed_acc:  80.5 & 95.7 & 86.2 & 86.2 & 88.2 & 95.5 & 81.6 & 91.7 & 76.1 & 87.6\n",
      "\n",
      "Epoch: 44, L: 0.3584, adv_l: 0.3689, T: 01:20 min, LR: 0.000523\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893590, Acc: 0.876230,  Time: 00:06 min, LR: 0.000497\n",
      "detailed_f1:  82.2 & 95.6 & 88.9 & 87.9 & 87.0 & 94.5 & 82.2 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  80.0 & 95.8 & 89.0 & 87.4 & 87.1 & 95.4 & 80.1 & 91.9 & 75.9 & 87.6\n",
      "\n",
      "Epoch: 45, L: 0.3570, adv_l: 0.3679, T: 01:22 min, LR: 0.000497\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893682, Acc: 0.876455,  Time: 00:06 min, LR: 0.000472\n",
      "detailed_f1:  82.3 & 95.5 & 89.1 & 87.9 & 87.0 & 94.6 & 82.2 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  81.3 & 95.7 & 88.1 & 87.8 & 89.3 & 95.5 & 81.2 & 91.0 & 75.7 & 87.6\n",
      "\n",
      "Epoch: 46, L: 0.3564, adv_l: 0.3675, T: 01:21 min, LR: 0.000472\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893574, Acc: 0.876230,  Time: 00:06 min, LR: 0.000449\n",
      "detailed_f1:  82.3 & 95.6 & 88.9 & 87.8 & 87.1 & 94.4 & 82.4 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.0 & 95.2 & 87.4 & 88.6 & 86.8 & 96.1 & 80.0 & 92.1 & 75.4 & 87.6\n",
      "\n",
      "Epoch: 47, L: 0.3557, adv_l: 0.3671, T: 01:20 min, LR: 0.000449\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893592, Acc: 0.876244,  Time: 00:06 min, LR: 0.000426\n",
      "detailed_f1:  82.4 & 95.6 & 88.9 & 87.8 & 87.2 & 94.4 & 82.4 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  82.1 & 95.3 & 87.5 & 86.5 & 87.3 & 95.9 & 79.9 & 92.1 & 74.8 & 87.6\n",
      "\n",
      "Epoch: 48, L: 0.3549, adv_l: 0.3665, T: 01:20 min, LR: 0.000426\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893652, Acc: 0.876328,  Time: 00:05 min, LR: 0.000405\n",
      "detailed_f1:  82.3 & 95.5 & 89.0 & 87.8 & 87.2 & 94.5 & 82.3 & 89.4 & 78.4 & 89.4\n",
      "detailed_acc:  78.9 & 94.8 & 87.3 & 85.8 & 88.5 & 95.5 & 80.1 & 92.8 & 75.8 & 87.6\n",
      "\n",
      "Epoch: 49, L: 0.3541, adv_l: 0.3660, T: 01:20 min, LR: 0.000405\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893567, Acc: 0.876131,  Time: 00:05 min, LR: 0.000385\n",
      "detailed_f1:  82.4 & 95.5 & 88.7 & 87.8 & 87.1 & 94.4 & 82.4 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  81.0 & 95.4 & 86.5 & 88.3 & 87.7 & 96.2 & 81.1 & 90.9 & 76.2 & 87.6\n",
      "\n",
      "Epoch: 50, L: 0.3541, adv_l: 0.3665, T: 01:21 min, LR: 0.000385\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893780, Acc: 0.876539,  Time: 00:06 min, LR: 0.000365\n",
      "detailed_f1:  82.3 & 95.6 & 88.9 & 88.0 & 87.3 & 94.4 & 82.5 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  81.8 & 95.0 & 87.5 & 87.4 & 87.4 & 96.1 & 81.6 & 91.3 & 75.3 & 87.7\n",
      "\n",
      "Epoch: 51, L: 0.3529, adv_l: 0.3653, T: 01:21 min, LR: 0.000365\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893603, Acc: 0.876370,  Time: 00:05 min, LR: 0.000347\n",
      "detailed_f1:  82.4 & 95.4 & 88.7 & 87.9 & 87.1 & 94.4 & 82.4 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  81.1 & 94.5 & 87.3 & 87.3 & 89.2 & 96.4 & 81.3 & 91.0 & 75.6 & 87.6\n",
      "\n",
      "Epoch: 52, L: 0.3526, adv_l: 0.3653, T: 01:20 min, LR: 0.000347\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893603, Acc: 0.876244,  Time: 00:06 min, LR: 0.000330\n",
      "detailed_f1:  82.3 & 95.5 & 88.9 & 87.8 & 87.1 & 94.5 & 82.3 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  78.8 & 95.1 & 88.9 & 87.7 & 87.4 & 95.6 & 81.5 & 92.5 & 74.7 & 87.6\n",
      "\n",
      "Epoch: 53, L: 0.3516, adv_l: 0.3647, T: 01:21 min, LR: 0.000330\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.894025, Acc: 0.876778,  Time: 00:06 min, LR: 0.000313\n",
      "detailed_f1:  82.4 & 95.6 & 88.9 & 87.9 & 87.1 & 94.5 & 82.3 & 89.6 & 78.5 & 89.4\n",
      "detailed_acc:  80.3 & 95.7 & 88.2 & 87.3 & 88.3 & 96.0 & 80.4 & 91.6 & 75.7 & 87.7\n",
      "\n",
      "Epoch: 54, L: 0.3513, adv_l: 0.3643, T: 01:20 min, LR: 0.000313\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893620, Acc: 0.876272,  Time: 00:06 min, LR: 0.000298\n",
      "detailed_f1:  82.3 & 95.5 & 89.0 & 87.9 & 87.1 & 94.5 & 82.3 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  79.9 & 95.7 & 88.1 & 87.1 & 88.3 & 95.8 & 80.6 & 92.2 & 74.8 & 87.6\n",
      "\n",
      "Epoch: 55, L: 0.3504, adv_l: 0.3640, T: 01:21 min, LR: 0.000298\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893364, Acc: 0.876216,  Time: 00:06 min, LR: 0.000283\n",
      "detailed_f1:  82.3 & 95.6 & 89.0 & 87.8 & 87.2 & 94.5 & 82.2 & 89.4 & 78.4 & 89.3\n",
      "detailed_acc:  80.6 & 95.1 & 87.2 & 86.4 & 88.4 & 95.4 & 79.8 & 92.6 & 75.2 & 87.6\n",
      "\n",
      "Epoch: 56, L: 0.3501, adv_l: 0.3636, T: 01:22 min, LR: 0.000283\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893304, Acc: 0.876019,  Time: 00:06 min, LR: 0.000269\n",
      "detailed_f1:  82.2 & 95.5 & 88.9 & 87.9 & 87.0 & 94.5 & 82.2 & 89.4 & 78.4 & 89.3\n",
      "detailed_acc:  79.9 & 95.2 & 88.1 & 87.4 & 87.7 & 95.4 & 81.4 & 91.7 & 75.8 & 87.6\n",
      "\n",
      "Epoch: 57, L: 0.3496, adv_l: 0.3632, T: 01:20 min, LR: 0.000269\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893397, Acc: 0.875850,  Time: 00:06 min, LR: 0.000255\n",
      "detailed_f1:  82.4 & 95.4 & 88.7 & 87.8 & 87.0 & 94.5 & 82.3 & 89.5 & 78.3 & 89.3\n",
      "detailed_acc:  79.5 & 95.1 & 86.8 & 87.4 & 88.5 & 95.8 & 80.5 & 91.8 & 75.9 & 87.6\n",
      "\n",
      "Epoch: 58, L: 0.3492, adv_l: 0.3631, T: 01:21 min, LR: 0.000255\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893358, Acc: 0.876019,  Time: 00:06 min, LR: 0.000242\n",
      "detailed_f1:  82.2 & 95.4 & 89.0 & 88.1 & 87.1 & 94.5 & 82.3 & 89.5 & 78.3 & 89.3\n",
      "detailed_acc:  81.3 & 95.7 & 87.5 & 87.6 & 88.6 & 95.7 & 80.3 & 91.5 & 75.4 & 87.6\n",
      "\n",
      "Epoch: 59, L: 0.3488, adv_l: 0.3630, T: 01:21 min, LR: 0.000242\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893350, Acc: 0.875892,  Time: 00:05 min, LR: 0.000230\n",
      "detailed_f1:  82.3 & 95.5 & 89.0 & 87.7 & 87.3 & 94.4 & 82.3 & 89.4 & 78.3 & 89.3\n",
      "detailed_acc:  80.2 & 94.9 & 88.5 & 86.6 & 88.5 & 95.3 & 79.6 & 92.3 & 75.5 & 87.6\n",
      "\n",
      "Epoch: 60, L: 0.3485, adv_l: 0.3627, T: 01:22 min, LR: 0.000230\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893990, Acc: 0.876609,  Time: 00:05 min, LR: 0.000219\n",
      "detailed_f1:  82.6 & 95.3 & 88.9 & 88.0 & 87.3 & 94.5 & 82.3 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  82.0 & 95.4 & 87.9 & 87.1 & 87.5 & 95.6 & 80.1 & 92.3 & 74.6 & 87.7\n",
      "\n",
      "Epoch: 61, L: 0.3479, adv_l: 0.3623, T: 01:22 min, LR: 0.000219\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893311, Acc: 0.875780,  Time: 00:06 min, LR: 0.000208\n",
      "detailed_f1:  82.3 & 95.5 & 89.0 & 87.7 & 87.1 & 94.5 & 82.4 & 89.4 & 78.1 & 89.3\n",
      "detailed_acc:  80.7 & 95.0 & 87.8 & 86.4 & 88.3 & 95.5 & 80.3 & 92.4 & 74.7 & 87.6\n",
      "\n",
      "Epoch: 62, L: 0.3475, adv_l: 0.3621, T: 01:22 min, LR: 0.000208\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893600, Acc: 0.876033,  Time: 00:06 min, LR: 0.000197\n",
      "detailed_f1:  82.1 & 95.5 & 88.9 & 87.8 & 87.2 & 94.6 & 82.5 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  80.9 & 95.7 & 88.4 & 87.4 & 88.3 & 95.3 & 81.0 & 91.8 & 75.1 & 87.6\n",
      "\n",
      "Epoch: 63, L: 0.3473, adv_l: 0.3622, T: 01:23 min, LR: 0.000197\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893739, Acc: 0.876412,  Time: 00:06 min, LR: 0.000188\n",
      "detailed_f1:  82.3 & 95.4 & 88.8 & 88.0 & 87.1 & 94.5 & 82.4 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  81.3 & 95.3 & 87.8 & 87.1 & 88.4 & 95.3 & 80.7 & 92.0 & 75.3 & 87.6\n",
      "\n",
      "Epoch: 64, L: 0.3469, adv_l: 0.3616, T: 01:22 min, LR: 0.000188\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893698, Acc: 0.876230,  Time: 00:05 min, LR: 0.000178\n",
      "detailed_f1:  82.3 & 95.4 & 88.9 & 87.9 & 87.2 & 94.6 & 82.3 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  79.8 & 94.6 & 88.0 & 86.8 & 87.8 & 95.4 & 81.2 & 91.9 & 76.2 & 87.6\n",
      "\n",
      "Epoch: 65, L: 0.3466, adv_l: 0.3616, T: 01:23 min, LR: 0.000178\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893842, Acc: 0.876609,  Time: 00:06 min, LR: 0.000169\n",
      "detailed_f1:  82.3 & 95.5 & 89.0 & 87.8 & 87.2 & 94.5 & 82.3 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  80.6 & 95.3 & 88.8 & 86.5 & 88.0 & 95.5 & 81.2 & 92.0 & 75.1 & 87.7\n",
      "\n",
      "Epoch: 66, L: 0.3464, adv_l: 0.3616, T: 01:23 min, LR: 0.000169\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893677, Acc: 0.876173,  Time: 00:05 min, LR: 0.000161\n",
      "detailed_f1:  82.2 & 95.4 & 89.0 & 88.0 & 86.9 & 94.5 & 82.5 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  80.7 & 95.7 & 88.1 & 87.4 & 88.2 & 94.9 & 81.3 & 91.5 & 76.3 & 87.6\n",
      "\n",
      "Epoch: 67, L: 0.3461, adv_l: 0.3613, T: 01:23 min, LR: 0.000161\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893717, Acc: 0.876188,  Time: 00:06 min, LR: 0.000153\n",
      "detailed_f1:  82.3 & 95.5 & 89.0 & 87.9 & 87.2 & 94.5 & 82.3 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  82.2 & 95.0 & 88.0 & 86.4 & 87.5 & 95.4 & 80.8 & 91.7 & 75.5 & 87.6\n",
      "\n",
      "Epoch: 68, L: 0.3455, adv_l: 0.3609, T: 01:22 min, LR: 0.000153\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893783, Acc: 0.876398,  Time: 00:06 min, LR: 0.000145\n",
      "detailed_f1:  82.4 & 95.4 & 89.0 & 87.8 & 87.1 & 94.5 & 82.4 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  81.2 & 95.3 & 87.9 & 86.8 & 88.4 & 95.5 & 80.7 & 92.0 & 75.0 & 87.6\n",
      "\n",
      "Epoch: 69, L: 0.3454, adv_l: 0.3608, T: 01:22 min, LR: 0.000145\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893698, Acc: 0.876440,  Time: 00:06 min, LR: 0.000138\n",
      "detailed_f1:  82.4 & 95.5 & 88.9 & 87.9 & 87.1 & 94.5 & 82.4 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.2 & 95.4 & 87.5 & 87.0 & 88.9 & 95.7 & 81.0 & 92.2 & 74.7 & 87.6\n",
      "\n",
      "Epoch: 70, L: 0.3454, adv_l: 0.3610, T: 01:21 min, LR: 0.000138\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.894048, Acc: 0.876750,  Time: 00:05 min, LR: 0.000131\n",
      "detailed_f1:  82.3 & 95.4 & 89.1 & 87.8 & 87.2 & 94.5 & 82.5 & 89.6 & 78.5 & 89.4\n",
      "detailed_acc:  80.9 & 95.2 & 87.8 & 87.3 & 88.2 & 95.4 & 80.8 & 91.5 & 76.4 & 87.7\n",
      "\n",
      "Epoch: 71, L: 0.3448, adv_l: 0.3604, T: 01:22 min, LR: 0.000131\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.894235, Acc: 0.876989,  Time: 00:06 min, LR: 0.000124\n",
      "detailed_f1:  82.5 & 95.5 & 89.0 & 87.9 & 87.2 & 94.5 & 82.4 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  81.7 & 95.0 & 88.2 & 86.2 & 88.4 & 95.6 & 80.6 & 91.9 & 75.3 & 87.7\n",
      "\n",
      "Epoch: 72, L: 0.3446, adv_l: 0.3603, T: 01:22 min, LR: 0.000124\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893581, Acc: 0.875892,  Time: 00:06 min, LR: 0.000118\n",
      "detailed_f1:  82.2 & 95.4 & 88.9 & 88.1 & 87.0 & 94.5 & 82.4 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  80.1 & 95.0 & 88.0 & 87.1 & 87.5 & 95.6 & 81.6 & 91.6 & 75.8 & 87.6\n",
      "\n",
      "Epoch: 73, L: 0.3444, adv_l: 0.3603, T: 01:23 min, LR: 0.000118\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.894326, Acc: 0.876778,  Time: 00:06 min, LR: 0.000112\n",
      "detailed_f1:  82.4 & 95.4 & 89.0 & 87.9 & 87.3 & 94.5 & 82.6 & 89.6 & 78.3 & 89.4\n",
      "detailed_acc:  80.6 & 95.4 & 87.5 & 86.8 & 88.3 & 95.4 & 81.4 & 91.8 & 75.9 & 87.7\n",
      "\n",
      "Epoch: 74, L: 0.3441, adv_l: 0.3600, T: 01:21 min, LR: 0.000112\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893950, Acc: 0.876679,  Time: 00:06 min, LR: 0.000107\n",
      "detailed_f1:  82.4 & 95.4 & 89.0 & 87.9 & 87.1 & 94.5 & 82.5 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  80.6 & 95.0 & 87.8 & 86.7 & 88.6 & 95.5 & 80.7 & 92.0 & 75.6 & 87.7\n",
      "\n",
      "Epoch: 75, L: 0.3439, adv_l: 0.3599, T: 01:21 min, LR: 0.000107\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893545, Acc: 0.876188,  Time: 00:06 min, LR: 0.000101\n",
      "detailed_f1:  82.3 & 95.4 & 88.9 & 87.9 & 87.3 & 94.5 & 82.4 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.2 & 95.3 & 88.6 & 86.6 & 88.6 & 95.2 & 81.5 & 91.9 & 75.4 & 87.6\n",
      "\n",
      "Epoch: 76, L: 0.3438, adv_l: 0.3600, T: 01:22 min, LR: 0.000101\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893887, Acc: 0.876581,  Time: 00:06 min, LR: 0.000096\n",
      "detailed_f1:  82.4 & 95.5 & 89.0 & 87.8 & 87.0 & 94.6 & 82.4 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  80.3 & 95.3 & 87.8 & 87.5 & 88.5 & 95.6 & 81.2 & 91.8 & 75.5 & 87.7\n",
      "\n",
      "Epoch: 77, L: 0.3437, adv_l: 0.3598, T: 01:21 min, LR: 0.000096\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893678, Acc: 0.876440,  Time: 00:06 min, LR: 0.000091\n",
      "detailed_f1:  82.2 & 95.4 & 89.1 & 87.9 & 87.0 & 94.5 & 82.4 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  80.5 & 95.0 & 87.6 & 86.8 & 88.3 & 95.7 & 81.7 & 91.7 & 75.7 & 87.6\n",
      "\n",
      "Epoch: 78, L: 0.3435, adv_l: 0.3597, T: 01:23 min, LR: 0.000091\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893741, Acc: 0.876300,  Time: 00:06 min, LR: 0.000087\n",
      "detailed_f1:  82.3 & 95.3 & 89.0 & 88.0 & 87.1 & 94.5 & 82.5 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.6 & 95.1 & 88.0 & 87.0 & 88.2 & 95.4 & 80.5 & 91.9 & 75.7 & 87.6\n",
      "\n",
      "Epoch: 79, L: 0.3435, adv_l: 0.3598, T: 01:22 min, LR: 0.000087\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893454, Acc: 0.875977,  Time: 00:06 min, LR: 0.000083\n",
      "detailed_f1:  82.3 & 95.5 & 88.7 & 88.0 & 87.0 & 94.4 & 82.3 & 89.5 & 78.2 & 89.3\n",
      "detailed_acc:  80.0 & 95.3 & 86.7 & 87.5 & 88.0 & 95.6 & 80.6 & 92.1 & 75.5 & 87.6\n",
      "\n",
      "Epoch: 80, L: 0.3431, adv_l: 0.3595, T: 01:22 min, LR: 0.000083\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893833, Acc: 0.876356,  Time: 00:06 min, LR: 0.000078\n",
      "detailed_f1:  82.4 & 95.5 & 89.1 & 87.8 & 87.2 & 94.5 & 82.5 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  80.8 & 95.0 & 88.3 & 86.4 & 87.7 & 95.3 & 81.4 & 91.6 & 76.2 & 87.6\n",
      "\n",
      "Epoch: 81, L: 0.3430, adv_l: 0.3593, T: 01:22 min, LR: 0.000078\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893858, Acc: 0.876314,  Time: 00:06 min, LR: 0.000075\n",
      "detailed_f1:  82.3 & 95.4 & 88.8 & 87.8 & 87.3 & 94.5 & 82.6 & 89.6 & 78.3 & 89.4\n",
      "detailed_acc:  80.2 & 95.6 & 87.5 & 87.4 & 88.5 & 95.3 & 81.3 & 91.8 & 75.9 & 87.6\n",
      "\n",
      "Epoch: 82, L: 0.3427, adv_l: 0.3593, T: 01:23 min, LR: 0.000075\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893665, Acc: 0.876356,  Time: 00:06 min, LR: 0.000071\n",
      "detailed_f1:  82.5 & 95.4 & 88.9 & 87.8 & 87.2 & 94.4 & 82.5 & 89.5 & 78.4 & 89.4\n",
      "detailed_acc:  81.1 & 95.3 & 88.5 & 87.2 & 88.3 & 95.5 & 81.2 & 91.6 & 75.4 & 87.6\n",
      "\n",
      "Epoch: 83, L: 0.3426, adv_l: 0.3591, T: 01:22 min, LR: 0.000071\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893576, Acc: 0.876061,  Time: 00:05 min, LR: 0.000067\n",
      "detailed_f1:  82.3 & 95.4 & 89.0 & 87.8 & 87.2 & 94.5 & 82.4 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  80.8 & 95.3 & 87.9 & 86.9 & 88.6 & 95.4 & 81.2 & 91.7 & 75.5 & 87.6\n",
      "\n",
      "Epoch: 84, L: 0.3426, adv_l: 0.3592, T: 01:22 min, LR: 0.000067\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893480, Acc: 0.876047,  Time: 00:06 min, LR: 0.000064\n",
      "detailed_f1:  82.2 & 95.6 & 88.7 & 87.8 & 87.2 & 94.5 & 82.6 & 89.4 & 78.3 & 89.3\n",
      "detailed_acc:  80.2 & 95.4 & 86.9 & 86.8 & 88.3 & 95.5 & 81.2 & 91.9 & 75.7 & 87.6\n",
      "\n",
      "Epoch: 85, L: 0.3424, adv_l: 0.3591, T: 01:22 min, LR: 0.000064\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893423, Acc: 0.875878,  Time: 00:05 min, LR: 0.000061\n",
      "detailed_f1:  82.1 & 95.5 & 88.9 & 87.9 & 87.1 & 94.5 & 82.4 & 89.5 & 78.2 & 89.3\n",
      "detailed_acc:  79.4 & 95.2 & 88.0 & 86.7 & 87.4 & 95.5 & 81.6 & 92.0 & 75.8 & 87.6\n",
      "\n",
      "Epoch: 86, L: 0.3424, adv_l: 0.3591, T: 01:23 min, LR: 0.000061\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893168, Acc: 0.875668,  Time: 00:06 min, LR: 0.000058\n",
      "detailed_f1:  82.2 & 95.4 & 88.6 & 87.9 & 87.1 & 94.5 & 82.4 & 89.5 & 78.2 & 89.3\n",
      "detailed_acc:  79.9 & 95.3 & 87.6 & 86.9 & 87.6 & 95.5 & 81.6 & 91.8 & 75.6 & 87.6\n",
      "\n",
      "Epoch: 87, L: 0.3422, adv_l: 0.3589, T: 01:23 min, LR: 0.000058\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893442, Acc: 0.876131,  Time: 00:06 min, LR: 0.000055\n",
      "detailed_f1:  82.3 & 95.4 & 89.1 & 87.8 & 87.3 & 94.5 & 82.3 & 89.4 & 78.2 & 89.3\n",
      "detailed_acc:  80.1 & 95.1 & 88.1 & 86.9 & 88.2 & 95.7 & 81.2 & 92.2 & 74.8 & 87.6\n",
      "\n",
      "Epoch: 88, L: 0.3421, adv_l: 0.3588, T: 01:22 min, LR: 0.000055\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893887, Acc: 0.876244,  Time: 00:04 min, LR: 0.000052\n",
      "detailed_f1:  82.3 & 95.4 & 89.0 & 87.9 & 87.2 & 94.5 & 82.5 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  80.9 & 94.9 & 88.2 & 86.6 & 88.0 & 95.4 & 80.9 & 91.7 & 75.9 & 87.6\n",
      "\n",
      "Epoch: 89, L: 0.3418, adv_l: 0.3589, T: 01:21 min, LR: 0.000052\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893991, Acc: 0.876679,  Time: 00:04 min, LR: 0.000049\n",
      "detailed_f1:  82.3 & 95.4 & 88.9 & 87.8 & 87.4 & 94.5 & 82.5 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  80.8 & 95.0 & 88.4 & 86.5 & 87.9 & 95.3 & 80.9 & 92.2 & 75.4 & 87.7\n",
      "\n",
      "Epoch: 90, L: 0.3419, adv_l: 0.3587, T: 01:21 min, LR: 0.000049\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893446, Acc: 0.875836,  Time: 00:04 min, LR: 0.000047\n",
      "detailed_f1:  82.3 & 95.4 & 88.9 & 87.8 & 87.1 & 94.5 & 82.4 & 89.4 & 78.2 & 89.3\n",
      "detailed_acc:  80.0 & 95.5 & 87.9 & 87.3 & 88.6 & 95.6 & 80.8 & 91.8 & 75.4 & 87.6\n",
      "\n",
      "Epoch: 91, L: 0.3418, adv_l: 0.3587, T: 01:22 min, LR: 0.000047\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893841, Acc: 0.876384,  Time: 00:05 min, LR: 0.000045\n",
      "detailed_f1:  82.4 & 95.4 & 89.0 & 87.8 & 87.3 & 94.5 & 82.4 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.8 & 95.1 & 87.9 & 87.0 & 88.2 & 95.5 & 81.3 & 91.8 & 75.3 & 87.6\n",
      "\n",
      "Epoch: 92, L: 0.3418, adv_l: 0.3588, T: 01:22 min, LR: 0.000045\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893853, Acc: 0.876272,  Time: 00:04 min, LR: 0.000042\n",
      "detailed_f1:  82.4 & 95.5 & 89.1 & 87.8 & 87.1 & 94.5 & 82.5 & 89.5 & 78.2 & 89.4\n",
      "detailed_acc:  80.9 & 95.3 & 87.6 & 87.5 & 87.8 & 95.7 & 80.8 & 91.9 & 75.1 & 87.6\n",
      "\n",
      "Epoch: 93, L: 0.3417, adv_l: 0.3585, T: 01:21 min, LR: 0.000042\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893945, Acc: 0.876497,  Time: 00:05 min, LR: 0.000040\n",
      "detailed_f1:  82.4 & 95.5 & 89.0 & 87.8 & 87.2 & 94.5 & 82.5 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.7 & 95.1 & 88.2 & 86.6 & 88.0 & 95.4 & 80.7 & 91.9 & 75.8 & 87.6\n",
      "\n",
      "Epoch: 94, L: 0.3415, adv_l: 0.3585, T: 01:21 min, LR: 0.000040\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893450, Acc: 0.875921,  Time: 00:05 min, LR: 0.000038\n",
      "detailed_f1:  82.2 & 95.5 & 89.0 & 87.8 & 87.0 & 94.5 & 82.4 & 89.5 & 78.2 & 89.3\n",
      "detailed_acc:  80.7 & 95.3 & 87.7 & 87.2 & 88.3 & 95.6 & 80.4 & 91.9 & 75.2 & 87.6\n",
      "\n",
      "Epoch: 95, L: 0.3415, adv_l: 0.3585, T: 01:21 min, LR: 0.000038\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893431, Acc: 0.876131,  Time: 00:05 min, LR: 0.000036\n",
      "detailed_f1:  82.2 & 95.4 & 88.9 & 87.9 & 87.1 & 94.5 & 82.5 & 89.5 & 78.3 & 89.3\n",
      "detailed_acc:  80.6 & 95.2 & 87.8 & 87.1 & 88.2 & 95.5 & 81.2 & 91.8 & 75.3 & 87.6\n",
      "\n",
      "Epoch: 96, L: 0.3413, adv_l: 0.3584, T: 01:21 min, LR: 0.000036\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893941, Acc: 0.876525,  Time: 00:05 min, LR: 0.000035\n",
      "detailed_f1:  82.3 & 95.4 & 89.0 & 88.0 & 87.1 & 94.5 & 82.4 & 89.6 & 78.4 & 89.4\n",
      "detailed_acc:  80.7 & 95.3 & 87.5 & 86.9 & 88.0 & 95.6 & 80.8 & 91.9 & 75.7 & 87.7\n",
      "\n",
      "Epoch: 97, L: 0.3414, adv_l: 0.3584, T: 01:20 min, LR: 0.000035\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893465, Acc: 0.876117,  Time: 00:05 min, LR: 0.000033\n",
      "detailed_f1:  82.4 & 95.4 & 89.0 & 87.7 & 87.2 & 94.5 & 82.5 & 89.4 & 78.3 & 89.3\n",
      "detailed_acc:  80.5 & 95.3 & 87.9 & 86.6 & 87.8 & 95.4 & 80.9 & 91.9 & 75.6 & 87.6\n",
      "\n",
      "Epoch: 98, L: 0.3412, adv_l: 0.3582, T: 01:21 min, LR: 0.000033\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893970, Acc: 0.876525,  Time: 00:05 min, LR: 0.000031\n",
      "detailed_f1:  82.6 & 95.4 & 88.9 & 87.7 & 87.2 & 94.5 & 82.5 & 89.5 & 78.3 & 89.4\n",
      "detailed_acc:  80.7 & 95.0 & 87.8 & 87.2 & 88.0 & 95.4 & 80.9 & 91.9 & 75.8 & 87.7\n",
      "\n",
      "Epoch: 99, L: 0.3411, adv_l: 0.3583, T: 01:20 min, LR: 0.000031\n",
      "Testing: \n",
      "Total Loss: 0.0001,  F1: 0.893306, Acc: 0.876103,  Time: 00:05 min, LR: 0.000030\n",
      "detailed_f1:  82.3 & 95.5 & 88.9 & 87.7 & 87.1 & 94.5 & 82.3 & 89.5 & 78.3 & 89.3\n",
      "detailed_acc:  80.8 & 95.4 & 88.0 & 86.8 & 88.3 & 95.5 & 80.6 & 92.0 & 75.1 & 87.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.95)\n",
    "\n",
    "weights_class = torch.Tensor(9).fill_(1)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_class).to(device)\n",
    "\n",
    "print('start ... ')\n",
    "for epoch in range(100):\n",
    "    train_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_adversarial_loss = 0\n",
    "    iteration_cnt = 0\n",
    "\n",
    "    for batch, (seeds, blocks) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        blocks = [b.to(device) for b in blocks]\n",
    "\n",
    "        # input\n",
    "        input_user = blocks[-1].srcdata['random_feature']['user']\n",
    "        input_instr = blocks[-1].srcdata['avg_instr_feature']['recipe']\n",
    "        input_ingredient = blocks[-1].srcdata['nutrient_feature']['ingredient']\n",
    "        input_image = blocks[-1].srcdata['resnet_image']['recipe']\n",
    "        labels = blocks[-1].dstdata['label']['recipe'] \n",
    "        inputs = [input_user, input_instr, input_ingredient, input_image]\n",
    "        \n",
    "        logits, x = model(blocks, inputs, seeds, None)\n",
    "        \n",
    "        # adversarial learning\n",
    "        adv_deltas = get_PGD_inputs(model, blocks, inputs, labels, seeds)\n",
    "        adv_logits, adv_x = model(blocks, inputs, seeds, adv_deltas)\n",
    "        \n",
    "        # compute loss\n",
    "        adversarial_loss = criterion(adv_logits, labels)\n",
    "        loss = criterion(logits, labels) + 0.1*adversarial_loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_adversarial_loss += adversarial_loss.item()\n",
    "        iteration_cnt += 1 \n",
    "#         break\n",
    "        \n",
    "    epoch_loss /= iteration_cnt\n",
    "    epoch_adversarial_loss /= iteration_cnt\n",
    "    train_end = time.strftime(\"%M:%S min\", time.gmtime(time.time()-train_start))\n",
    "    \n",
    "    print('Epoch: {0}, L: {l:.4f}, adv_l: {adv_l:.4f}, T: {t}, LR: {lr:.6f}'\n",
    "          .format(epoch, l=epoch_loss, adv_l=epoch_adversarial_loss, t=train_end, lr=opt.param_groups[0]['lr']))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluation\n",
    "    # For demonstration purpose, only test set result is reported here. Please use val_dataloader for comprehensiveness.\n",
    "    test_loss, test_f1, test_acc, test_time, test_detailed_f1, test_detailed_acc, link_prediction_test_loss \\\n",
    "    = evaluate(model, test_dataloader, device)\n",
    "    \n",
    "    print('Testing: ')\n",
    "    print('Total Loss: {l:.4f},  F1: {f1:.6f}, Acc: {acc:.6f},  Time: {t}, LR: {lr:.6f}'\n",
    "          .format(l=test_loss, f1=test_f1, acc=test_acc, t=test_time, lr=opt.param_groups[0]['lr']))\n",
    "    \n",
    "    # show in overleaf table structure\n",
    "    test_detailed_f1 = [str('{:.1f}'.format(i*100)) for i in list(test_detailed_f1)]\n",
    "    test_detailed_acc = [str('{:.1f}'.format(i*100)) for i in list(test_detailed_acc)]\n",
    "    print('detailed_f1: ', ' & '.join(test_detailed_f1[1:]) + ' & ' + test_detailed_f1[0] + ' & ' + '{:.1f}'.format(test_f1*100))\n",
    "    print('detailed_acc: ', ' & '.join(test_detailed_acc[1:]) + ' & ' + test_detailed_acc[0] + ' & ' + '{:.1f}'.format(test_acc*100))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919db19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79906f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e4363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaf180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
